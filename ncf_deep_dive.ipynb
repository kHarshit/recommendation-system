{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXu5yHVg_HQK"
      },
      "source": [
        "<i>Copyright (c) Recommenders contributors.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij0XXQUV_HQM"
      },
      "source": [
        "# Neural Collaborative Filtering (NCF)\n",
        "\n",
        "This notebook serves as an introduction to Neural Collaborative Filtering (NCF), which is an innovative algorithm based on deep neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT-YTOYn_HQM"
      },
      "source": [
        "## 0 Global Settings and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_recommenders"
      ],
      "metadata": {
        "id": "W-npXo_j_Owp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recommenders"
      ],
      "metadata": {
        "id": "O6exZ6cJBmAl",
        "outputId": "ac8ea60b-d643-4734-a80f-b6682365cddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recommenders\n",
            "  Downloading recommenders-1.2.0-py3-none-any.whl (356 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.0/356.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting category-encoders<3,>=2.6.0 (from recommenders)\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cornac<2,>=1.15.2 (from recommenders)\n",
            "  Downloading cornac-1.18.0-cp310-cp310-manylinux1_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hyperopt<1,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from recommenders) (0.2.7)\n",
            "Collecting lightfm<2,>=1.17 (from recommenders)\n",
            "  Downloading lightfm-1.17.tar.gz (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.4/316.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lightgbm<5,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (4.1.0)\n",
            "Collecting locust<3,>=2.12.2 (from recommenders)\n",
            "  Downloading locust-2.27.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting memory-profiler<1,>=0.61.0 (from recommenders)\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: nltk<4,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from recommenders) (3.8.1)\n",
            "Collecting notebook<8,>=7.0.0 (from recommenders)\n",
            "  Downloading notebook-7.2.0-py3-none-any.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba<1,>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (0.58.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>2.0.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (2.0.3)\n",
            "Collecting retrying<2,>=1.3.4 (from recommenders)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (1.2.2)\n",
            "Collecting scikit-surprise>=1.1.3 (from recommenders)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from recommenders) (1.11.4)\n",
            "Requirement already satisfied: seaborn<1,>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (0.13.1)\n",
            "Requirement already satisfied: transformers<5,>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from recommenders) (4.41.0)\n",
            "Collecting pandera[strategies]>=0.15.0 (from recommenders)\n",
            "  Downloading pandera-0.19.3-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.9/251.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.25.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (0.14.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (0.5.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cornac<2,>=1.15.2->recommenders) (4.66.4)\n",
            "Collecting powerlaw (from cornac<2,>=1.15.2->recommenders)\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (0.10.9.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lightfm<2,>=1.17->recommenders) (2.31.0)\n",
            "Collecting gevent>=22.10.2 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from locust<3,>=2.12.2->recommenders) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from locust<3,>=2.12.2->recommenders) (3.0.3)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from locust<3,>=2.12.2->recommenders) (1.0.8)\n",
            "Collecting pyzmq>=25.0.0 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading pyzmq-26.0.3-cp310-cp310-manylinux_2_28_x86_64.whl (919 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.8/919.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geventhttpclient==2.2.1 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading geventhttpclient-2.2.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ConfigArgParse>=1.5.5 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.10/dist-packages (from locust<3,>=2.12.2->recommenders) (5.9.5)\n",
            "Collecting Flask-Login>=0.6.3 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
            "Collecting Flask-Cors>=3.0.10 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from locust<3,>=2.12.2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from geventhttpclient==2.2.1->locust<3,>=2.12.2->recommenders) (2024.2.2)\n",
            "Collecting brotli (from geventhttpclient==2.2.1->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from geventhttpclient==2.2.1->locust<3,>=2.12.2->recommenders) (2.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.8.1->recommenders) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.8.1->recommenders) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.8.1->recommenders) (2023.12.25)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyter_server-2.14.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.3/383.3 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-server<3,>=2.27.1 (from notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyterlab_server-2.27.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab<4.3,>=4.2.0 (from notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyterlab-4.2.0-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from notebook<8,>=7.0.0->recommenders) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from notebook<8,>=7.0.0->recommenders) (6.3.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba<1,>=0.57.0->recommenders) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2024.1)\n",
            "Collecting multimethod<=1.10.0 (from pandera[strategies]>=0.15.0->recommenders)\n",
            "  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pandera[strategies]>=0.15.0->recommenders) (24.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from pandera[strategies]>=0.15.0->recommenders) (2.7.1)\n",
            "Collecting typeguard (from pandera[strategies]>=0.15.0->recommenders)\n",
            "  Downloading typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
            "Collecting typing-inspect>=0.6.0 (from pandera[strategies]>=0.15.0->recommenders)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pandera[strategies]>=0.15.0->recommenders) (1.14.1)\n",
            "Collecting hypothesis>=6.92.7 (from pandera[strategies]>=0.15.0->recommenders)\n",
            "  Downloading hypothesis-6.102.4-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1.2.0->recommenders) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn<1,>=0.13.0->recommenders) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.27.0->recommenders) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.27.0->recommenders) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.4.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (2.2.0)\n",
            "Collecting zope.event (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope.interface-6.4.post0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=22.10.2->locust<3,>=2.12.2->recommenders) (3.0.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers<5,>=4.27.0->recommenders) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers<5,>=4.27.0->recommenders) (4.11.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0->recommenders) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0->recommenders) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0->recommenders) (1.2.1)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (23.1.0)\n",
            "Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.7.2)\n",
            "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.20.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (5.7.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.8.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Collecting httpx>=0.25.0 (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel>=6.5.0 (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-lsp>=2.0.0 (from jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (2.15.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (4.19.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm<2,>=1.17->recommenders) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm<2,>=1.17->recommenders) (3.7)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.6.0->pandera[strategies]>=0.15.0->recommenders)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug>=2.0.0->locust<3,>=2.12.2->recommenders) (2.1.5)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.10/dist-packages (from powerlaw->cornac<2,>=1.15.2->recommenders) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->pandera[strategies]>=0.15.0->recommenders) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->pandera[strategies]>=0.15.0->recommenders) (2.18.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (21.2.0)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (1.6.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.2.2)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.19.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent>=22.10.2->locust<3,>=2.12.2->recommenders) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (3.0.43)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (4.9.0)\n",
            "Collecting fqdn (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders) (1.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook<8,>=7.0.0->recommenders) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook<8,>=7.0.0->recommenders) (0.2.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook<8,>=7.0.0->recommenders)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: lightfm, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.17-cp310-cp310-linux_x86_64.whl size=806106 sha256=310d2c611fea248a9356d6708fbf777bf9236cb292582a66fd961d3cf718889b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/9b/7e/0b256f2168511d8fa4dae4fae0200fdbd729eb424a912ad636\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357247 sha256=fa16d1ff01a190974d3ec90d04a10dc9b5380718e217ba99ce3195a0c40ac644\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built lightfm scikit-surprise\n",
            "Installing collected packages: brotli, zope.interface, zope.event, uri-template, types-python-dateutil, typeguard, rfc3986-validator, rfc3339-validator, retrying, pyzmq, python-json-logger, overrides, mypy-extensions, multimethod, memory-profiler, jsonpointer, json5, jedi, hypothesis, h11, fqdn, ConfigArgParse, comm, async-lru, typing-inspect, scikit-surprise, jupyter-server-terminals, jupyter-client, httpcore, gevent, arrow, powerlaw, pandera, lightfm, isoduration, ipykernel, httpx, geventhttpclient, Flask-Login, Flask-Cors, locust, cornac, category-encoders, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, recommenders\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.5\n",
            "    Uninstalling notebook-6.5.5:\n",
            "      Successfully uninstalled notebook-6.5.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 7.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ConfigArgParse-1.7 Flask-Cors-4.0.1 Flask-Login-0.6.3 arrow-1.3.0 async-lru-2.0.4 brotli-1.1.0 category-encoders-2.6.3 comm-0.2.2 cornac-1.18.0 fqdn-1.5.1 gevent-24.2.1 geventhttpclient-2.2.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 hypothesis-6.102.4 ipykernel-6.29.4 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-2.4 jupyter-client-8.6.1 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.0 jupyter-server-terminals-0.5.3 jupyterlab-4.2.0 jupyterlab-server-2.27.2 lightfm-1.17 locust-2.27.0 memory-profiler-0.61.0 multimethod-1.10 mypy-extensions-1.0.0 notebook-7.2.0 overrides-7.7.0 pandera-0.19.3 powerlaw-1.5 python-json-logger-2.0.7 pyzmq-26.0.3 recommenders-1.2.0 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scikit-surprise-1.1.4 typeguard-4.2.1 types-python-dateutil-2.9.0.20240316 typing-inspect-0.9.0 uri-template-1.3.0 zope.event-5.0 zope.interface-6.4.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "6s5zrR0I_t0r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iyfFPTfg_HQN",
        "outputId": "bfef1dae-52b6-4233-b8e9-f6ba542496c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "Pandas version: 2.0.3\n",
            "Tensorflow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.models.ncf.ncf_singlenode import NCF\n",
        "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
        "from recommenders.datasets import movielens\n",
        "from recommenders.datasets.python_splitters import python_chrono_split\n",
        "from recommenders.evaluation.python_evaluation import (\n",
        "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
        ")\n",
        "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
        "from recommenders.utils.notebook_utils import store_metadata\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "Itj8zzog_HQN"
      },
      "outputs": [],
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'\n",
        "\n",
        "# Model parameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjR_-eAC_HQO"
      },
      "source": [
        "## 1 Matrix factorization algorithm\n",
        "\n",
        "NCF is a neural matrix factorization model, which ensembles Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to unify the strengths of linearity of MF and non-linearity of MLP for modelling the user–item latent structures. NCF can be demonstrated as a framework for GMF and MLP, which is illustrated as below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7TlpzSy_HQO"
      },
      "source": [
        "<img src=\"https://recodatasets.z20.web.core.windows.net/images/NCF.svg?sanitize=true\">\n",
        "\n",
        "This figure shows how to utilize latent vectors of items and users, and then how to fuse outputs from GMF Layer (left) and MLP Layer (right). We will introduce this framework and show how to learn the model parameters in following sections.\n",
        "\n",
        "### 1.1 The GMF model\n",
        "\n",
        "In ALS, the ratings are modeled as follows:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
        "\n",
        "GMF introduces a neural CF layer as the output layer of standard MF. In this way, MF can be easily generalized\n",
        "and extended. For example, if we allow the edge weights of this output layer to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for activation, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. GMF can be shown as follows:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
        "\n",
        "where $\\odot$ is element-wise product of vectors. Additionally, ${a}_{out}$ and ${h}$ denote the activation function and edge weights of the output layer respectively. MF can be interpreted as a special case of GMF. Intuitively, if we use an identity function for ${a}_{out}$ and enforce ${h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n",
        "\n",
        "### 1.2 The MLP model\n",
        "\n",
        "NCF adopts two pathways to model users and items: 1) element-wise product of vectors, 2) concatenation of vectors. To learn interactions after concatenating of users and items latent features, the standard MLP model is applied. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between $p_{u}$ and $q_{i}$. The details of MLP model are:\n",
        "\n",
        "For the input layer, there is concatenation of user and item vectors:\n",
        "\n",
        "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
        "\n",
        "So for the hidden layers and output layer of MLP, the details are:\n",
        "\n",
        "$$\n",
        "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
        "$$\n",
        "\n",
        "and:\n",
        "\n",
        "$$\n",
        "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
        "$$\n",
        "\n",
        "where ${ W }_{ l }$, ${ b }_{ l }$, and ${ a }_{ out }$ denote the weight matrix, bias vector, and activation function for the $l$-th layer’s perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. Because we have a binary classification task, the activation function of the output layer is defined as sigmoid $\\sigma(x)=\\frac{1}{1+e^{-x}}$ to restrict the predicted score to be in (0,1).\n",
        "\n",
        "\n",
        "### 1.3 Fusion of GMF and MLP\n",
        "\n",
        "To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. We get $\\phi^{GMF}$ from GMF:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
        "\n",
        "and obtain $\\phi^{MLP}$ from MLP:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
        "\n",
        "Lastly, we fuse output from GMF and MLP:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
        "\n",
        "This model combines the linearity of MF and non-linearity of DNNs for modelling user–item latent structures.\n",
        "\n",
        "### 1.4 Objective Function\n",
        "\n",
        "We define the likelihood function as:\n",
        "\n",
        "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
        "\n",
        "Where $\\mathcal{R}$ denotes the set of observed interactions, and $\\mathcal{ R } ^ { - }$ denotes the set of negative instances. $\\mathbf{P}$ and $\\mathbf{Q}$ denotes the latent factor matrix for users and items, respectively; and $\\Theta$ denotes the model parameters. Taking the negative logarithm of the likelihood, we obtain the objective function to minimize for NCF method, which is known as [binary cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy):\n",
        "\n",
        "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$\n",
        "\n",
        "The optimization can be done by performing Stochastic Gradient Descent (SGD), which is described in the [Surprise SVD deep dive notebook](surprise_svd_deep_dive.ipynb). Our SGD method is very similar to the SVD algorithm's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ue9T_yg_HQO"
      },
      "source": [
        "## 2 TensorFlow implementation of NCF\n",
        "\n",
        "We will use the MovieLens dataset, which is composed of integer ratings from 1 to 5.\n",
        "\n",
        "We convert MovieLens into implicit feedback, and evaluate under our *leave-one-out* evaluation protocol.\n",
        "\n",
        "You can check the details of implementation in `recommenders/models/ncf`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apN1GwMI_HQP"
      },
      "source": [
        "## 3 TensorFlow NCF movie recommender\n",
        "\n",
        "### 3.1 Load and split data\n",
        "\n",
        "To evaluate the performance of item recommendation, we adopt the leave-one-out evaluation.\n",
        "\n",
        "For each user, we held out his/her last interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. Our test samples will be constructed by `NCFDataset`.\n",
        "\n",
        "We also show an alternative evaluation method, splitting the data chronologically using `python_chrono_split` to achieve a 75/25% training and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ar5GK_87_HQP",
        "outputId": "73c53c04-3f09-4351-f034-19747e8c1c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:02<00:00, 2.28kKB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userID  itemID  rating  timestamp\n",
              "0     196     242     3.0  881250949\n",
              "1     186     302     3.0  891717742\n",
              "2      22     377     1.0  878887116\n",
              "3     244      51     2.0  880606923\n",
              "4     166     346     1.0  886397596"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18ce04db-65b4-4cf9-b378-e3b7c2770e75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2.0</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18ce04db-65b4-4cf9-b378-e3b7c2770e75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18ce04db-65b4-4cf9-b378-e3b7c2770e75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18ce04db-65b4-4cf9-b378-e3b7c2770e75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a75904d2-066f-4510-87d6-e96344e64530\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a75904d2-066f-4510-87d6-e96344e64530')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a75904d2-066f-4510-87d6-e96344e64530 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"userID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 266,\n        \"min\": 1,\n        \"max\": 943,\n        \"num_unique_values\": 943,\n        \"samples\": [\n          262,\n          136,\n          821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 330,\n        \"min\": 1,\n        \"max\": 1682,\n        \"num_unique_values\": 1682,\n        \"samples\": [\n          1557,\n          808,\n          1618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.125673599144316,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0,\n          5.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5343856,\n        \"min\": 874724710,\n        \"max\": 893286638,\n        \"num_unique_values\": 49282,\n        \"samples\": [\n          889728713,\n          888443306,\n          880605158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = movielens.load_pandas_df(\n",
        "    size=MOVIELENS_DATA_SIZE,\n",
        "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3LWqbt82_HQP"
      },
      "outputs": [],
      "source": [
        "train, test = python_chrono_split(df, 0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNrAAan_HQQ"
      },
      "source": [
        "Filter out any users or items in the test set that do not appear in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zz7Aucec_HQQ"
      },
      "outputs": [],
      "source": [
        "test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n",
        "test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lN9anC_HQQ"
      },
      "source": [
        "Create a test set containing the last interaction for each user as for the leave-one-out evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OyN-L_yF_HQQ"
      },
      "outputs": [],
      "source": [
        "leave_one_out_test = test.groupby(\"userID\").last().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avbC3VV-_HQQ"
      },
      "source": [
        "Write datasets to csv files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pzQklCxQ_HQQ"
      },
      "outputs": [],
      "source": [
        "train_file = \"./train.csv\"\n",
        "test_file = \"./test.csv\"\n",
        "leave_one_out_test_file = \"./leave_one_out_test.csv\"\n",
        "train.to_csv(train_file, index=False)\n",
        "test.to_csv(test_file, index=False)\n",
        "leave_one_out_test.to_csv(leave_one_out_test_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5_zSp65_HQR"
      },
      "source": [
        "### 3.2 Functions of NCF Dataset\n",
        "\n",
        "Important functions of the Dataset class for NCF:\n",
        "\n",
        "`train_loader(batch_size, shuffle_size)`, generate training batches of size `batch_size`. Positive examples are loaded from the training file and negative samples are added in memory. `shuffle_size` determines the number of rows that are read into memory before the examples are shuffled. By default, the function will attempt to load all data before performing the shuffle. If memory constraints are encountered when using large datasets, try reducing `shuffle_size`.\n",
        "\n",
        "`test_loader()`, generate test batch by every positive test instance, (eg. `[1, 2, 1]` is a positive user & item pair in test set (`[userID, itemID, rating]` for this tuple). This function returns data like `[[1, 2, 1], [1, 3, 0], [1,6, 0], ...]`, ie. following our *leave-one-out* evaluation protocol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NO_7ikNq_HQR",
        "outputId": "9cd4b5c4-ffbe-4d1e-97ef-d95edacfeb16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 943/943 [00:08<00:00, 115.08it/s]\n"
          ]
        }
      ],
      "source": [
        "data = NCFDataset(train_file=train_file, test_file=leave_one_out_test_file, seed=SEED, overwrite_test_file_full=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcsWR2v_HQR"
      },
      "source": [
        "### 3.3 Train NCF based on TensorFlow\n",
        "The NCF has a lot of parameters. The most important ones are:\n",
        "\n",
        "`n_factors`, which controls the dimension of the latent space. Usually, the quality of the training set predictions grows with as n_factors gets higher.\n",
        "\n",
        "`layer_sizes`, sizes of input layer (and hidden layers) of MLP, input type is list.\n",
        "\n",
        "`n_epochs`, which defines the number of iteration of the SGD procedure.\n",
        "Note that both parameter also affect the training time.\n",
        "\n",
        "`model_type`, we can train single `\"MLP\"`, `\"GMF\"` or combined model `\"NCF\"` by changing the type of model.\n",
        "\n",
        "We will here set `n_factors` to 4, `layer_sizes` to `[16,8,4]`,  `n_epochs` to 100, `batch_size` to 256. To train the model, we simply need to call the `fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lo7l5zHu_HQR",
        "outputId": "4d163044-57b9-4e4b-957d-dab9f680b076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "model = NCF(\n",
        "    n_users=data.n_users,\n",
        "    n_items=data.n_items,\n",
        "    model_type=\"NeuMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uafHJim__HQR",
        "outputId": "4be692b0-20ab-4096-f2b8-8c52120ea643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 793.214526516 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZevGU5E3_HQR"
      },
      "source": [
        "## 3.4 Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dcxWj91_HQR"
      },
      "source": [
        "### 3.4.1 Prediction\n",
        "\n",
        "Now that our model is fitted, we can call `predict` to get some `predictions`. `predict` returns an internal object Prediction which can be easily converted back to a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TP1AC6QW_HQR",
        "outputId": "d4581b07-f0d3-4b95-d9aa-947b0c6782a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userID  itemID  prediction\n",
              "0     1.0   149.0    0.000483\n",
              "1     1.0    88.0    0.895971\n",
              "2     1.0   103.0    0.024878\n",
              "3     1.0   239.0    0.863149\n",
              "4     1.0   110.0    0.107989"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-062a0e43-4745-4b3f-a1ed-d33925ead3e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>0.000483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.895971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>0.024878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>0.863149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.107989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-062a0e43-4745-4b3f-a1ed-d33925ead3e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-062a0e43-4745-4b3f-a1ed-d33925ead3e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-062a0e43-4745-4b3f-a1ed-d33925ead3e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bab148af-b047-4dab-9191-70ffe7643604\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bab148af-b047-4dab-9191-70ffe7643604')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bab148af-b047-4dab-9191-70ffe7643604 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "predictions",
              "summary": "{\n  \"name\": \"predictions\",\n  \"rows\": 24891,\n  \"fields\": [\n    {\n      \"column\": \"userID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 266.70674008073814,\n        \"min\": 1.0,\n        \"max\": 943.0,\n        \"num_unique_values\": 943,\n        \"samples\": [\n          97.0,\n          266.0,\n          811.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 354.69403896619446,\n        \"min\": 1.0,\n        \"max\": 1664.0,\n        \"num_unique_values\": 1457,\n        \"samples\": [\n          117.0,\n          1517.0,\n          908.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3057267841090351,\n        \"min\": 2.0463373019765586e-18,\n        \"max\": 0.9979221224784851,\n        \"num_unique_values\": 24881,\n        \"samples\": [\n          0.02783864550292492,\n          0.03090646117925644,\n          0.9502882957458496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n",
        "               for (_, row) in test.iterrows()]\n",
        "\n",
        "\n",
        "predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0I-edoF_HQS"
      },
      "source": [
        "### 3.4.2 Generic Evaluation\n",
        "We remove rated movies in the top k recommendations\n",
        "To compute ranking metrics, we need predictions on all user, item pairs. We remove though the items already watched by the user, since we choose not to recommend them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gyPd1Tsy_HQS",
        "outputId": "96a1a80d-0553-4887-d33f-f271e2c11bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 7.733116421999966 seconds for prediction.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    users, items, preds = [], [], []\n",
        "    item = list(train.itemID.unique())\n",
        "    for user in train.userID.unique():\n",
        "        user = [user] * len(item)\n",
        "        users.extend(user)\n",
        "        items.extend(item)\n",
        "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
        "\n",
        "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "\n",
        "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
        "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tzK2FgmB_HQS",
        "outputId": "5bbf53cd-0512-499c-8bd1-1988ac1f3877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP:\t0.047740\n",
            "NDCG:\t0.198133\n",
            "Precision@K:\t0.178791\n",
            "Recall@K:\t0.101153\n"
          ]
        }
      ],
      "source": [
        "eval_map = map(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "\n",
        "print(\"MAP:\\t%f\" % eval_map,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Precision@K:\\t%f\" % eval_precision,\n",
        "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFMXx6Uw_HQS"
      },
      "source": [
        "### 3.4.3 \"Leave-one-out\" Evaluation\n",
        "\n",
        "We implement the functions to repoduce the leave-one-out evaluation protocol mentioned in original NCF paper.\n",
        "\n",
        "For each item in test data, we randomly samples 100 items that are not interacted by the user, ranking the test item among the 101 items (1 positive item and 100 negative items). The performance of a ranked list is judged by **Hit Ratio (HR)** and **Normalized Discounted Cumulative Gain (NDCG)**. Finally, we average the values of those ranked lists to obtain the overall HR and NDCG on test data.\n",
        "\n",
        "We truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ojH5ghsA_HQS",
        "outputId": "7d8930cb-358f-4ad1-b8c4-e95a9533ad80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HR:\t0.527041\n",
            "NDCG:\t0.415763\n"
          ]
        }
      ],
      "source": [
        "k = TOP_K\n",
        "\n",
        "ndcgs = []\n",
        "hit_ratio = []\n",
        "\n",
        "for b in data.test_loader():\n",
        "    user_input, item_input, labels = b\n",
        "    output = model.predict(user_input, item_input, is_list=True)\n",
        "\n",
        "    output = np.squeeze(output)\n",
        "    rank = sum(output >= output[0])\n",
        "    if rank <= k:\n",
        "        ndcgs.append(1 / np.log(rank + 1))\n",
        "        hit_ratio.append(1)\n",
        "    else:\n",
        "        ndcgs.append(0)\n",
        "        hit_ratio.append(0)\n",
        "\n",
        "eval_ndcg = np.mean(ndcgs)\n",
        "eval_hr = np.mean(hit_ratio)\n",
        "\n",
        "print(\"HR:\\t%f\" % eval_hr)\n",
        "print(\"NDCG:\\t%f\" % eval_ndcg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0J7UszE_HQS"
      },
      "source": [
        "## 3.5 Pre-training\n",
        "\n",
        "To get better performance of NeuMF, we can adopt pre-training strategy. We first train GMF and MLP with random initializations until convergence. Then use their model parameters as the initialization for the corresponding parts of NeuMF’s parameters.  Please pay attention to the output layer, where we concatenate weights of the two models with\n",
        "\n",
        "$$h ^ { N C F } \\leftarrow \\left[ \\begin{array} { c } { \\alpha h ^ { G M F } } \\\\ { ( 1 - \\alpha ) h ^ { M L P } } \\end{array} \\right]$$\n",
        "\n",
        "where $h^{GMF}$ and $h^{MLP}$ denote the $h$ vector of the pretrained GMF and MLP model, respectively; and $\\alpha$ is a\n",
        "hyper-parameter determining the trade-off between the two pre-trained models. We set $\\alpha$ = 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vyx6ZBY_HQT"
      },
      "source": [
        "### 3.5.1 Training GMF and MLP model\n",
        "`model.save`, we can set the `dir_name` to store the parameters of GMF and MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pHvxuahn_HQT",
        "outputId": "1f18d56e-fcc3-4cae-ea0b-3438b97bb942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "model = NCF(\n",
        "    n_users=data.n_users,\n",
        "    n_items=data.n_items,\n",
        "    model_type=\"GMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-ttGZXG-_HQT",
        "outputId": "4f0c7c6c-3384-4d1b-ef29-cf4998247b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 638.5558302219999 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "model.save(dir_name=\".pretrain/GMF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ueDRb5qK_HQT",
        "outputId": "5c150a05-8df8-48bb-8ede-7d05a96d4300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "model = NCF(\n",
        "    n_users=data.n_users,\n",
        "    n_items=data.n_items,\n",
        "    model_type=\"MLP\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZG0RYuUk_HQT",
        "outputId": "bc944161-8a9f-4cd5-84ad-1984b8ebd5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 671.0447422059997 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "model.save(dir_name=\".pretrain/MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8yKLMud_HQU"
      },
      "source": [
        "### 3.5.2 Load pre-trained GMF and MLP model for NeuMF\n",
        "`model.load`, we can set the `gmf_dir` and `mlp_dir` to store the parameters for NeuMF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FZ3g9qxv_HQU"
      },
      "outputs": [],
      "source": [
        "model = NCF(\n",
        "    n_users=data.n_users,\n",
        "    n_items=data.n_items,\n",
        "    model_type=\"NeuMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "model.load(gmf_dir=\".pretrain/GMF\", mlp_dir=\".pretrain/MLP\", alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kgLOdp69_HQU",
        "outputId": "b90afd60-6d55-4061-f4b5-6181c37bcfa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 794.8690407929998 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOCKIFiQ_HQU"
      },
      "source": [
        "### 3.5.3 Compare with not pre-trained NeuMF\n",
        "\n",
        "You can use beforementioned evaluation methods to evaluate the pre-trained `NCF` Model. Usually, we will find the performance of pre-trained NCF is better than the not pre-trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xWtGt1BJ_HQU",
        "outputId": "f9eb8438-a30f-49fc-c0f2-83f415924696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 8.369522309999866 seconds for prediction.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    users, items, preds = [], [], []\n",
        "    item = list(train.itemID.unique())\n",
        "    for user in train.userID.unique():\n",
        "        user = [user] * len(item)\n",
        "        users.extend(user)\n",
        "        items.extend(item)\n",
        "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
        "\n",
        "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "\n",
        "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
        "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VqT0_lY5_HQU",
        "outputId": "a1a8e05a-8020-4904-f6fd-aa3add302c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP:\t0.046457\n",
            "NDCG:\t0.185886\n",
            "Precision@K:\t0.165854\n",
            "Recall@K:\t0.097339\n"
          ]
        }
      ],
      "source": [
        "eval_map2 = map(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_ndcg2 = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_precision2 = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_recall2 = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "\n",
        "print(\"MAP:\\t%f\" % eval_map2,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg2,\n",
        "      \"Precision@K:\\t%f\" % eval_precision2,\n",
        "      \"Recall@K:\\t%f\" % eval_recall2, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "trlBBbIF_HQV",
        "outputId": "01d353ec-736a-48c1-8268-c259af285cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "map",
              "data": 0.04774028533024079,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "map",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "ndcg",
              "data": 0.41576285035600713,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "ndcg",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "precision",
              "data": 0.1787910922587487,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "precision",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "recall",
              "data": 0.10115299771243305,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "recall",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "map2",
              "data": 0.04645739287673663,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "map2",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "ndcg2",
              "data": 0.18588634087722383,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "ndcg2",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "precision2",
              "data": 0.1658536585365854,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "precision2",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "recall2",
              "data": 0.09733929345382206,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "recall2",
              "data": true,
              "display": false
            }
          }
        }
      ],
      "source": [
        "# Record results for tests - ignore this cell\n",
        "store_metadata(\"map\", eval_map)\n",
        "store_metadata(\"ndcg\", eval_ndcg)\n",
        "store_metadata(\"precision\", eval_precision)\n",
        "store_metadata(\"recall\", eval_recall)\n",
        "store_metadata(\"map2\", eval_map2)\n",
        "store_metadata(\"ndcg2\", eval_ndcg2)\n",
        "store_metadata(\"precision2\", eval_precision2)\n",
        "store_metadata(\"recall2\", eval_recall2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2JJTtWq_HQV"
      },
      "source": [
        "### 3.5.4 Delete pre-trained directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRUyrh7n_HQV"
      },
      "outputs": [],
      "source": [
        "# save_dir = \".pretrain\"\n",
        "# if os.path.exists(save_dir):\n",
        "#     shutil.rmtree(save_dir)\n",
        "\n",
        "# print(\"Did \\'%s\\' exist?: %s\" % (save_dir, os.path.exists(save_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HFaq1Q_HQV"
      },
      "source": [
        "### Reference:\n",
        "1. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017, https://arxiv.org/abs/1708.05031\n",
        "\n",
        "2. Official NCF implementation [Keras with Theano]: https://github.com/hexiangnan/neural_collaborative_filtering\n",
        "\n",
        "3. Other nice NCF implementation [Pytorch]: https://github.com/LaceyChen17/neural-collaborative-filtering"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "interpreter": {
      "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
    },
    "kernelspec": {
      "display_name": "reco_gpu",
      "language": "python",
      "name": "conda-env-reco_gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}